{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"SMSSpamCollection.csv\" , sep=\"\\t\" , header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=[\"label\" , \"body_text\"]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56749f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts(normalize=True).plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data['msg_len'] = data['body_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=data, x='msg_len', hue='label', bins=50, kde=True)\n",
    "plt.title(\"Distribution of Message Length (Spam vs Ham)\")\n",
    "plt.xlabel(\"Message Length (words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d6707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f9b94",
   "metadata": {},
   "source": [
    "3. Data Preparation-Pre-processing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe7c43",
   "metadata": {},
   "source": [
    "2. ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜÿµŸàÿµ (Text Cleaning)\n",
    "\n",
    "ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ≠ÿ±ŸàŸÅ ŸÑÿµŸäÿ∫ÿ© ŸÖŸàÿ≠ÿØÿ© (Lowercasing).\n",
    "\n",
    "ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ (ŸÑŸà ŸÖÿ¥ ŸÖŸáŸÖÿ©).\n",
    "\n",
    "ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿÆÿßÿµÿ© (punctuations, emojis ŸÑŸà ŸÖÿ¥ ŸÖÿ≠ÿ™ÿßÿ¨Ÿáÿß).\n",
    "\n",
    "ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ÿßŸÑÿ≤ÿßÿ¶ÿØÿ©.\n",
    "\n",
    "üîπ 3. ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿπÿØŸäŸÖÿ© ÿßŸÑŸÇŸäŸÖÿ© (Stopwords Removal)\n",
    "\n",
    "ŸÉŸÑŸÖÿßÿ™ ÿ≤Ÿä \"the, is, in, ŸÖŸÜÿå ÿπŸÑŸâ\" ÿ∫ÿßŸÑÿ®Ÿãÿß ŸÖÿ¥ ÿ®ÿ™ÿ∂ŸäŸÅ ŸÖÿπŸÜŸâ ŸÉÿ®Ÿäÿ±.\n",
    "\n",
    "üîπ 4. ÿ™ÿ¨ÿ≤ÿ¶ÿ© ÿßŸÑŸÜÿµ (Tokenization)\n",
    "\n",
    "ÿ™ŸÇÿ≥ŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ŸÉŸÑŸÖÿßÿ™ ÿ£Ÿà ÿ¨ŸèŸÖŸÑ.\n",
    "\n",
    "ŸÖÿ´ÿßŸÑ: \"I love NLP\" ‚Üí [\"I\", \"love\", \"NLP\"].\n",
    "\n",
    "üîπ 5. ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÑÿµŸäÿ∫Ÿáÿß ÿßŸÑÿ¨ÿ∞ÿ±Ÿäÿ© ÿ£Ÿà ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "Stemming: ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÉŸÑŸÖÿ© ŸÑÿ¨ÿ∞ÿ±Ÿáÿß (e.g., \"playing\" ‚Üí \"play\").\n",
    "\n",
    "Lemmatization: ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÉŸÑŸÖÿ© ŸÑÿµŸäÿ∫ÿ™Ÿáÿß ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© (e.g., \"better\" ‚Üí \"good\").\n",
    "\n",
    "üîπ 6. ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸÜÿµ ŸÑÿ™ŸÖÿ´ŸäŸÑ ÿπÿØÿØŸä (Feature Extraction)\n",
    "\n",
    "ÿπÿ¥ÿßŸÜ ÿßŸÑŸÖŸàÿØŸäŸÑ ŸäŸÅŸáŸÖÿå ÿßŸÑŸÜÿµ ŸÑÿßÿ≤ŸÖ Ÿäÿ™ÿ≠ŸàŸÑ ŸÑÿ£ÿ±ŸÇÿßŸÖ:\n",
    "\n",
    "Bag of Words (BoW)\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "\n",
    "Transformers Embeddings (BERT, GPT ‚Ä¶)\n",
    "\n",
    "üîπ 7. ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿ∫Ÿäÿ± ŸÖÿ™Ÿàÿßÿ≤ŸÜÿ© ÿ£Ÿà ÿ∑ŸàŸäŸÑÿ©\n",
    "\n",
    "Padding/Truncating: ÿπÿ¥ÿßŸÜ ÿßŸÑŸÜÿµŸàÿµ ŸäŸÉŸàŸÜ ŸÑŸáÿß ŸÜŸÅÿ≥ ÿßŸÑÿ∑ŸàŸÑ.\n",
    "\n",
    "Oversampling/Undersampling: ŸÑŸà ÿπŸÜÿØŸÉ ŸÉŸÑÿßÿ≥ ŸÖŸàÿ≤ŸàŸÜ ÿ®ÿ¥ŸÉŸÑ ÿ≥Ÿäÿ°.\n",
    "\n",
    "üîπ 8. ÿßŸÑÿ™ŸÇÿ≥ŸäŸÖ (Train-Test Split)\n",
    "\n",
    "ÿ™ŸÇÿ≥ŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸÄ:\n",
    "\n",
    "ÿ™ÿØÿ±Ÿäÿ® (Train)\n",
    "\n",
    "ÿ™ÿ≠ŸÇŸÇ (Validation)\n",
    "\n",
    "ÿßÿÆÿ™ÿ®ÿßÿ± (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f20c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc76e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#\\W+ regex, indicates that it will split wherever it sees one or more non-word characters.\n",
    "#So that'll split on white spaces, special characters, anything like that.\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "data['body_text_tokenized'] = data['body_text_nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7abd49",
   "metadata": {},
   "source": [
    "3.4 Remove stopwords\n",
    "Stopwords are common words that are present in the text but generally do not contribute to the meaning of a sentence. They hold almost no importance for the purposes of information retrieval and natural language processing. They can safely be ignored without sacrificing the meaning of the sentence. For example ‚Äì ‚Äòthe‚Äô and ‚Äòa‚Äô.\n",
    "\n",
    "Stop Words: A stop word is a commonly used word (such as ‚Äúthe‚Äù, ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúin‚Äù) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "The NLTK package has a separate package of stop words that can be downloaded. NLTK has stop words more than 16 languages which can be downloaded and used. Once it is downloaded, it can be passed as an argument indicating it to ignore these words.\n",
    "\n",
    "import nltk  from nltk.corpus \n",
    "import stopwords  set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_En = nltk.corpus.stopwords.words('english')\n",
    "stopwords_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37754f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_En]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bfe40",
   "metadata": {},
   "source": [
    "üîπ 1. Stemming\n",
    "\n",
    "ŸáŸà ÿπŸÖŸÑŸäÿ© ŸÇÿµ ÿ£Ÿà ÿ™ŸÇÿ∑Ÿäÿπ ÿßŸÑŸÉŸÑŸÖÿ© ÿπÿ¥ÿßŸÜ ŸÜŸàÿµŸÑ ŸÑÿ¨ÿ∞ÿ±Ÿáÿß (Root)ÿå ŸÑŸÉŸÜ ÿ®ÿ¥ŸÉŸÑ ÿ®ÿ≥Ÿäÿ∑ Ÿàÿ≥ÿ±Ÿäÿπ ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß ŸäŸáÿ™ŸÖ ÿ®ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑŸÑÿ∫ŸàŸä.\n",
    "\n",
    "ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿ≥ÿßÿπÿßÿ™ ÿ®ÿ™ŸÉŸàŸÜ ŸÉŸÑŸÖÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅÿπŸÑŸäŸãÿß ŸÅŸä ÿßŸÑŸÇÿßŸÖŸàÿ≥.\n",
    "\n",
    "‚úÖ ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "\"Studies\" ‚Üí \"Studi\"\n",
    "\n",
    "\"Studying\" ‚Üí \"Study\" ÿ£Ÿà ÿ£ÿ≠ŸäÿßŸÜŸãÿß \"Studi\"\n",
    "\n",
    "\"Better\" ‚Üí \"Better\" (ŸÖŸÖŸÉŸÜ Ÿäÿ≥Ÿäÿ®Ÿáÿß ÿ≤Ÿä ŸÖÿß ŸáŸä ŸÑÿ£ŸÜ ÿßŸÑŸÄ stemmer ŸÖÿ¥ ŸÅÿßŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ)\n",
    "\n",
    "üìå ÿ£ÿØÿßÿ© ŸÖÿ¥ŸáŸàÿ±ÿ©: Porter Stemmer ÿ£Ÿà Snowball Stemmer ŸÅŸä ŸÖŸÉÿ™ÿ®ÿ© NLTK.\n",
    "\n",
    "ŸÖŸäÿ≤ÿ©: ÿ£ÿ≥ÿ±ÿπ.\n",
    "\n",
    "ÿπŸäÿ®: ŸÖŸÖŸÉŸÜ ÿ™ÿ∑ŸÑÿπ ŸÜÿ™ÿßÿ¶ÿ¨ ŸÖÿ¥ ÿØŸÇŸäŸÇÿ© ÿ£Ÿà ŸÉŸÑŸÖÿßÿ™ ŸÖÿßŸÑŸáÿßÿ¥ ŸÖÿπŸÜŸâ.\n",
    "\n",
    "üîπ 2. Lemmatization\n",
    "\n",
    "ÿ®ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑŸÇÿßŸÖŸàÿ≥ (Dictionary) + ÿßŸÑŸÇŸàÿßÿπÿØ ÿßŸÑŸÑÿ∫ŸàŸäÿ© (Morphology).\n",
    "\n",
    "ÿ®ÿ™ÿØŸäŸÉ ÿßŸÑÿ¨ÿ∞ÿ± ÿßŸÑÿµÿ≠Ÿäÿ≠ ŸÑŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÑŸä ŸÖŸàÿ¨ŸàÿØ ŸÅÿπŸÑŸãÿß ŸÅŸä ÿßŸÑŸÇÿßŸÖŸàÿ≥.\n",
    "\n",
    "ÿ®ÿ™ÿßÿÆÿØ ŸÅŸä ÿßÿπÿ™ÿ®ÿßÿ±Ÿáÿß ŸÜŸàÿπ ÿßŸÑŸÉŸÑŸÖÿ© (POS Tag: verb, noun, adjective, ...).\n",
    "\n",
    "‚úÖ ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "\"Studies\" ‚Üí \"Study\" (ÿßÿ≥ŸÖ ÿ£Ÿà ŸÅÿπŸÑ)\n",
    "\n",
    "\"Studying\" ‚Üí \"Study\"\n",
    "\n",
    "\"Better\" ‚Üí \"Good\" (ŸÑÿ£ŸÜŸáÿß ŸÅÿßŸáŸÖÿ© ÿ•ŸÜ \"better\" ŸáŸä ÿµŸäÿ∫ÿ© ÿ™ŸÅÿ∂ŸäŸÑ ŸÖŸÜ \"good\")\n",
    "\n",
    "üìå ÿ£ÿØÿßÿ© ŸÖÿ¥ŸáŸàÿ±ÿ©: WordNet Lemmatizer ŸÅŸä NLTK ÿ£Ÿà spacy Lemmatizer.\n",
    "\n",
    "ŸÖŸäÿ≤ÿ©: ÿ£ÿØŸÇ.\n",
    "\n",
    "ÿπŸäÿ®: ÿ£ÿ®ÿ∑ÿ£ ŸÖŸÜ stemming ŸÑÿ£ŸÜŸáÿß ŸÖÿ≠ÿ™ÿßÿ¨ÿ© ŸÇÿßÿπÿØÿ© ÿ®ŸäÿßŸÜÿßÿ™ ŸÑÿ∫ŸàŸäÿ©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6630041",
   "metadata": {},
   "source": [
    "üîë ÿßŸÑÿÆŸÑÿßÿµÿ©:\n",
    "\n",
    "ŸÑŸà ÿπÿßŸäÿ≤ ÿ≥ÿ±ÿπÿ© (ÿ≤Ÿä ŸÅŸä search engines) ‚Üí ÿßÿ≥ÿ™ÿÆÿØŸÖ Stemming.\n",
    "\n",
    "ŸÑŸà ÿπÿßŸäÿ≤ ÿØŸÇÿ© ŸàŸÖÿπŸÜŸâ ÿµÿ≠Ÿäÿ≠ (ÿ≤Ÿä NLP applications ÿ£Ÿà chatbots) ‚Üí ÿßÿ≥ÿ™ÿÆÿØŸÖ Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3298c",
   "metadata": {},
   "source": [
    "lemmatizer=>ÿßÿØŸÇ ŸÅ ÿßŸÑŸÖÿπŸÜŸä "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_lemma(text):\n",
    "\n",
    "    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text_lemma(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem=ÿßÿ≥ÿ±ÿπ\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15023f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc28bd",
   "metadata": {},
   "source": [
    "üîë ÿßŸÑÿÆŸÑÿßÿµÿ©:\n",
    "\n",
    "ŸÑŸà ŸÖÿ¥ÿ±Ÿàÿπ ÿ®ÿ≥Ÿäÿ∑ (ÿ™ÿµŸÜŸäŸÅ Spam/Not Spam ŸÖÿ´ŸÑÿßŸã) ‚Üí ÿßÿ≥ÿ™ÿÆÿØŸÖ TF-IDF.\n",
    "\n",
    "ŸÑŸà ŸÖÿ¥ÿ±Ÿàÿπ ÿπÿßŸäÿ≤ ŸÖÿπŸÜŸâ ÿ£ÿπŸÖŸÇ (Sentiment Analysis, Chatbot, Translation) ‚Üí ÿßÿ≥ÿ™ÿÆÿØŸÖ Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c50b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "#tfidf = TfidfVectorizer(min_df=1)\n",
    "tfidf = TfidfVectorizer( ngram_range=(2,2))\n",
    "features_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n', features_tfidf)\n",
    "features_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
    "\n",
    "features_tfidf.columns = tfidf.get_feature_names_out()\n",
    "features_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sample_features = features_tfidf[:, :30].toarray()\n",
    "\n",
    "features_df = pd.DataFrame(sample_features, columns=tfidf.get_feature_names_out()[:30])\n",
    "\n",
    "\n",
    "corr = np.corrcoef(features_df.T)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", \n",
    "            xticklabels=tfidf.get_feature_names_out()[:30], \n",
    "            yticklabels=tfidf.get_feature_names_out()[:30])\n",
    "plt.title(\"Correlation Heatmap of Top 30 TF-IDF Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52581526",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "X = tfidf.fit_transform(data['cleaned_text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = features_tfidf\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7711a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Comparison\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=list(results.keys()), y=list(results.values()), palette=\"viridis\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = \" \".join(data[data['label']=='spam']['cleaned_text'])\n",
    "ham_words = \" \".join(data[data['label']=='ham']['cleaned_text'])\n",
    "\n",
    "spam_wc = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(spam_words)\n",
    "ham_wc = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate(ham_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(spam_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Spam WordCloud\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(ham_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ham WordCloud\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b466df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = nb.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test.map({'ham':0,'spam':1}), y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1], color='navy', lw=2, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Naive Bayes\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd36f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sv = LinearSVC()\n",
    "sv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_scores = sv.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test.map({'ham':0,'spam':1}), y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label=f\"SVM (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - SVM\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sv1 = LogisticRegression(max_iter=200)\n",
    "sv1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_scores = sv1.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test.map({'ham':0,'spam':1}), y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label=f\"SVM (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - logisticRegression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "joblib.dump(nb, \"spam_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
