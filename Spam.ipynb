{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"SMSSpamCollection.csv\" , sep=\"\\t\" , header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=[\"label\" , \"body_text\"]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56749f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts(normalize=True).plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data['msg_len'] = data['body_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=data, x='msg_len', hue='label', bins=50, kde=True)\n",
    "plt.title(\"Distribution of Message Length (Spam vs Ham)\")\n",
    "plt.xlabel(\"Message Length (words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d6707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f9b94",
   "metadata": {},
   "source": [
    "3. Data Preparation-Pre-processing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe7c43",
   "metadata": {},
   "source": [
    "2. تنظيف النصوص (Text Cleaning)\n",
    "\n",
    "تحويل الحروف لصيغة موحدة (Lowercasing).\n",
    "\n",
    "إزالة الأرقام (لو مش مهمة).\n",
    "\n",
    "إزالة الرموز الخاصة (punctuations, emojis لو مش محتاجها).\n",
    "\n",
    "إزالة المسافات الزائدة.\n",
    "\n",
    "🔹 3. إزالة الكلمات عديمة القيمة (Stopwords Removal)\n",
    "\n",
    "كلمات زي \"the, is, in, من، على\" غالبًا مش بتضيف معنى كبير.\n",
    "\n",
    "🔹 4. تجزئة النص (Tokenization)\n",
    "\n",
    "تقسم النص إلى كلمات أو جُمل.\n",
    "\n",
    "مثال: \"I love NLP\" → [\"I\", \"love\", \"NLP\"].\n",
    "\n",
    "🔹 5. تحويل الكلمات لصيغها الجذرية أو الأساسية\n",
    "\n",
    "Stemming: إرجاع الكلمة لجذرها (e.g., \"playing\" → \"play\").\n",
    "\n",
    "Lemmatization: إرجاع الكلمة لصيغتها الصحيحة (e.g., \"better\" → \"good\").\n",
    "\n",
    "🔹 6. تحويل النص لتمثيل عددي (Feature Extraction)\n",
    "\n",
    "عشان الموديل يفهم، النص لازم يتحول لأرقام:\n",
    "\n",
    "Bag of Words (BoW)\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "\n",
    "Transformers Embeddings (BERT, GPT …)\n",
    "\n",
    "🔹 7. التعامل مع النصوص الغير متوازنة أو طويلة\n",
    "\n",
    "Padding/Truncating: عشان النصوص يكون لها نفس الطول.\n",
    "\n",
    "Oversampling/Undersampling: لو عندك كلاس موزون بشكل سيء.\n",
    "\n",
    "🔹 8. التقسيم (Train-Test Split)\n",
    "\n",
    "تقسم البيانات لـ:\n",
    "\n",
    "تدريب (Train)\n",
    "\n",
    "تحقق (Validation)\n",
    "\n",
    "اختبار (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f20c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc76e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#\\W+ regex, indicates that it will split wherever it sees one or more non-word characters.\n",
    "#So that'll split on white spaces, special characters, anything like that.\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "data['body_text_tokenized'] = data['body_text_nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7abd49",
   "metadata": {},
   "source": [
    "3.4 Remove stopwords\n",
    "Stopwords are common words that are present in the text but generally do not contribute to the meaning of a sentence. They hold almost no importance for the purposes of information retrieval and natural language processing. They can safely be ignored without sacrificing the meaning of the sentence. For example – ‘the’ and ‘a’.\n",
    "\n",
    "Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "The NLTK package has a separate package of stop words that can be downloaded. NLTK has stop words more than 16 languages which can be downloaded and used. Once it is downloaded, it can be passed as an argument indicating it to ignore these words.\n",
    "\n",
    "import nltk  from nltk.corpus \n",
    "import stopwords  set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_En = nltk.corpus.stopwords.words('english')\n",
    "stopwords_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37754f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_En]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bfe40",
   "metadata": {},
   "source": [
    "🔹 1. Stemming\n",
    "\n",
    "هو عملية قص أو تقطيع الكلمة عشان نوصل لجذرها (Root)، لكن بشكل بسيط وسريع من غير ما يهتم بالمعنى اللغوي.\n",
    "\n",
    "النتيجة ساعات بتكون كلمة غير موجودة فعليًا في القاموس.\n",
    "\n",
    "✅ مثال:\n",
    "\n",
    "\"Studies\" → \"Studi\"\n",
    "\n",
    "\"Studying\" → \"Study\" أو أحيانًا \"Studi\"\n",
    "\n",
    "\"Better\" → \"Better\" (ممكن يسيبها زي ما هي لأن الـ stemmer مش فاهم المعنى)\n",
    "\n",
    "📌 أداة مشهورة: Porter Stemmer أو Snowball Stemmer في مكتبة NLTK.\n",
    "\n",
    "ميزة: أسرع.\n",
    "\n",
    "عيب: ممكن تطلع نتائج مش دقيقة أو كلمات مالهاش معنى.\n",
    "\n",
    "🔹 2. Lemmatization\n",
    "\n",
    "بتعتمد على القاموس (Dictionary) + القواعد اللغوية (Morphology).\n",
    "\n",
    "بتديك الجذر الصحيح للكلمة اللي موجود فعلًا في القاموس.\n",
    "\n",
    "بتاخد في اعتبارها نوع الكلمة (POS Tag: verb, noun, adjective, ...).\n",
    "\n",
    "✅ مثال:\n",
    "\n",
    "\"Studies\" → \"Study\" (اسم أو فعل)\n",
    "\n",
    "\"Studying\" → \"Study\"\n",
    "\n",
    "\"Better\" → \"Good\" (لأنها فاهمة إن \"better\" هي صيغة تفضيل من \"good\")\n",
    "\n",
    "📌 أداة مشهورة: WordNet Lemmatizer في NLTK أو spacy Lemmatizer.\n",
    "\n",
    "ميزة: أدق.\n",
    "\n",
    "عيب: أبطأ من stemming لأنها محتاجة قاعدة بيانات لغوية."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6630041",
   "metadata": {},
   "source": [
    "🔑 الخلاصة:\n",
    "\n",
    "لو عايز سرعة (زي في search engines) → استخدم Stemming.\n",
    "\n",
    "لو عايز دقة ومعنى صحيح (زي NLP applications أو chatbots) → استخدم Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3298c",
   "metadata": {},
   "source": [
    "lemmatizer=>ادق ف المعني "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_lemma(text):\n",
    "\n",
    "    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text_lemma(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem=اسرع\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15023f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc28bd",
   "metadata": {},
   "source": [
    "🔑 الخلاصة:\n",
    "\n",
    "لو مشروع بسيط (تصنيف Spam/Not Spam مثلاً) → استخدم TF-IDF.\n",
    "\n",
    "لو مشروع عايز معنى أعمق (Sentiment Analysis, Chatbot, Translation) → استخدم Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c50b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "#tfidf = TfidfVectorizer(min_df=1)\n",
    "tfidf = TfidfVectorizer( ngram_range=(2,2))\n",
    "features_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n', features_tfidf)\n",
    "features_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
    "\n",
    "features_tfidf.columns = tfidf.get_feature_names_out()\n",
    "features_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sample_features = features_tfidf[:, :30].toarray()\n",
    "\n",
    "features_df = pd.DataFrame(sample_features, columns=tfidf.get_feature_names_out()[:30])\n",
    "\n",
    "\n",
    "corr = np.corrcoef(features_df.T)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", \n",
    "            xticklabels=tfidf.get_feature_names_out()[:30], \n",
    "            yticklabels=tfidf.get_feature_names_out()[:30])\n",
    "plt.title(\"Correlation Heatmap of Top 30 TF-IDF Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52581526",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "X = tfidf.fit_transform(data['cleaned_text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = features_tfidf\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7711a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Comparison\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=list(results.keys()), y=list(results.values()), palette=\"viridis\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = \" \".join(data[data['label']=='spam']['cleaned_text'])\n",
    "ham_words = \" \".join(data[data['label']=='ham']['cleaned_text'])\n",
    "\n",
    "spam_wc = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(spam_words)\n",
    "ham_wc = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate(ham_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(spam_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Spam WordCloud\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(ham_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ham WordCloud\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b466df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = nb.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test.map({'ham':0,'spam':1}), y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1], color='navy', lw=2, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Naive Bayes\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd36f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sv = LinearSVC()\n",
    "sv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_scores = sv.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test.map({'ham':0,'spam':1}), y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label=f\"SVM (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - SVM\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sv1 = LogisticRegression(max_iter=200)\n",
    "sv1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_scores = sv1.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test.map({'ham':0,'spam':1}), y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label=f\"SVM (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - logisticRegression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "joblib.dump(nb, \"spam_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
