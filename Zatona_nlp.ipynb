{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"SMSSpamCollection.csv\" , sep=\"\\t\" , header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=[\"label\" , \"body_text\"]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56749f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts(normalize=True).plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f9b94",
   "metadata": {},
   "source": [
    "3. Data Preparation-Pre-processing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe7c43",
   "metadata": {},
   "source": [
    "2. تنظيف النصوص (Text Cleaning)\n",
    "\n",
    "تحويل الحروف لصيغة موحدة (Lowercasing).\n",
    "\n",
    "إزالة الأرقام (لو مش مهمة).\n",
    "\n",
    "إزالة الرموز الخاصة (punctuations, emojis لو مش محتاجها).\n",
    "\n",
    "إزالة المسافات الزائدة.\n",
    "\n",
    "🔹 3. إزالة الكلمات عديمة القيمة (Stopwords Removal)\n",
    "\n",
    "كلمات زي \"the, is, in, من، على\" غالبًا مش بتضيف معنى كبير.\n",
    "\n",
    "🔹 4. تجزئة النص (Tokenization)\n",
    "\n",
    "تقسم النص إلى كلمات أو جُمل.\n",
    "\n",
    "مثال: \"I love NLP\" → [\"I\", \"love\", \"NLP\"].\n",
    "\n",
    "🔹 5. تحويل الكلمات لصيغها الجذرية أو الأساسية\n",
    "\n",
    "Stemming: إرجاع الكلمة لجذرها (e.g., \"playing\" → \"play\").\n",
    "\n",
    "Lemmatization: إرجاع الكلمة لصيغتها الصحيحة (e.g., \"better\" → \"good\").\n",
    "\n",
    "🔹 6. تحويل النص لتمثيل عددي (Feature Extraction)\n",
    "\n",
    "عشان الموديل يفهم، النص لازم يتحول لأرقام:\n",
    "\n",
    "Bag of Words (BoW)\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "\n",
    "Transformers Embeddings (BERT, GPT …)\n",
    "\n",
    "🔹 7. التعامل مع النصوص الغير متوازنة أو طويلة\n",
    "\n",
    "Padding/Truncating: عشان النصوص يكون لها نفس الطول.\n",
    "\n",
    "Oversampling/Undersampling: لو عندك كلاس موزون بشكل سيء.\n",
    "\n",
    "🔹 8. التقسيم (Train-Test Split)\n",
    "\n",
    "تقسم البيانات لـ:\n",
    "\n",
    "تدريب (Train)\n",
    "\n",
    "تحقق (Validation)\n",
    "\n",
    "اختبار (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f20c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprhansion \n",
    "#lambda function\n",
    "#adding join to join chars into words \n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lower to teh remove_punc function\n",
    "\n",
    "#list comprhansion \n",
    "#lambda function\n",
    "#adding join to join chars into words \n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc76e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#\\W+ regex, indicates that it will split wherever it sees one or more non-word characters.\n",
    "#So that'll split on white spaces, special characters, anything like that.\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "data['body_text_tokenized'] = data['body_text_nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7abd49",
   "metadata": {},
   "source": [
    "3.4 Remove stopwords\n",
    "Stopwords are common words that are present in the text but generally do not contribute to the meaning of a sentence. They hold almost no importance for the purposes of information retrieval and natural language processing. They can safely be ignored without sacrificing the meaning of the sentence. For example – ‘the’ and ‘a’.\n",
    "\n",
    "Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "The NLTK package has a separate package of stop words that can be downloaded. NLTK has stop words more than 16 languages which can be downloaded and used. Once it is downloaded, it can be passed as an argument indicating it to ignore these words.\n",
    "\n",
    "import nltk  from nltk.corpus \n",
    "import stopwords  set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_En = nltk.corpus.stopwords.words('english')\n",
    "stopwords_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37754f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_En]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bfe40",
   "metadata": {},
   "source": [
    "🔹 1. Stemming\n",
    "\n",
    "هو عملية قص أو تقطيع الكلمة عشان نوصل لجذرها (Root)، لكن بشكل بسيط وسريع من غير ما يهتم بالمعنى اللغوي.\n",
    "\n",
    "النتيجة ساعات بتكون كلمة غير موجودة فعليًا في القاموس.\n",
    "\n",
    "✅ مثال:\n",
    "\n",
    "\"Studies\" → \"Studi\"\n",
    "\n",
    "\"Studying\" → \"Study\" أو أحيانًا \"Studi\"\n",
    "\n",
    "\"Better\" → \"Better\" (ممكن يسيبها زي ما هي لأن الـ stemmer مش فاهم المعنى)\n",
    "\n",
    "📌 أداة مشهورة: Porter Stemmer أو Snowball Stemmer في مكتبة NLTK.\n",
    "\n",
    "ميزة: أسرع.\n",
    "\n",
    "عيب: ممكن تطلع نتائج مش دقيقة أو كلمات مالهاش معنى.\n",
    "\n",
    "🔹 2. Lemmatization\n",
    "\n",
    "بتعتمد على القاموس (Dictionary) + القواعد اللغوية (Morphology).\n",
    "\n",
    "بتديك الجذر الصحيح للكلمة اللي موجود فعلًا في القاموس.\n",
    "\n",
    "بتاخد في اعتبارها نوع الكلمة (POS Tag: verb, noun, adjective, ...).\n",
    "\n",
    "✅ مثال:\n",
    "\n",
    "\"Studies\" → \"Study\" (اسم أو فعل)\n",
    "\n",
    "\"Studying\" → \"Study\"\n",
    "\n",
    "\"Better\" → \"Good\" (لأنها فاهمة إن \"better\" هي صيغة تفضيل من \"good\")\n",
    "\n",
    "📌 أداة مشهورة: WordNet Lemmatizer في NLTK أو spacy Lemmatizer.\n",
    "\n",
    "ميزة: أدق.\n",
    "\n",
    "عيب: أبطأ من stemming لأنها محتاجة قاعدة بيانات لغوية."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6630041",
   "metadata": {},
   "source": [
    "🔑 الخلاصة:\n",
    "\n",
    "لو عايز سرعة (زي في search engines) → استخدم Stemming.\n",
    "\n",
    "لو عايز دقة ومعنى صحيح (زي NLP applications أو chatbots) → استخدم Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3298c",
   "metadata": {},
   "source": [
    "lemmatizer=>ادق ف المعني "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_lemma(text):\n",
    "    # 1. إزالة علامات الترقيم وتحويل لحروف صغيرة\n",
    "    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    # 2. تقسيم النص لكلمات\n",
    "    tokens = word_tokenize(text)\n",
    "    # 3. إزالة stopwords + Lemmatization\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text_lemma(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem=اسرع\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     #tokens = re.split('\\W+', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15023f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc28bd",
   "metadata": {},
   "source": [
    "🔑 الخلاصة:\n",
    "\n",
    "لو مشروع بسيط (تصنيف Spam/Not Spam مثلاً) → استخدم TF-IDF.\n",
    "\n",
    "لو مشروع عايز معنى أعمق (Sentiment Analysis, Chatbot, Translation) → استخدم Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c50b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "#tfidf = TfidfVectorizer(min_df=1)\n",
    "tfidf = TfidfVectorizer( ngram_range=(2,2))\n",
    "features_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n', features_tfidf)\n",
    "features_tfidf = pd.DataFrame(features_tfidf.toarray())\n",
    "features_tfidf.columns = tfidf.get_feature_names_out()\n",
    "features_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
