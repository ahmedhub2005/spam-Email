{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"SMSSpamCollection.csv\" , sep=\"\\t\" , header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=[\"label\" , \"body_text\"]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56749f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts(normalize=True).plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f9b94",
   "metadata": {},
   "source": [
    "3. Data Preparation-Pre-processing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe7c43",
   "metadata": {},
   "source": [
    "2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ (Text Cleaning)\n",
    "\n",
    "ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ù„ØµÙŠØºØ© Ù…ÙˆØ­Ø¯Ø© (Lowercasing).\n",
    "\n",
    "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (Ù„Ùˆ Ù…Ø´ Ù…Ù‡Ù…Ø©).\n",
    "\n",
    "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© (punctuations, emojis Ù„Ùˆ Ù…Ø´ Ù…Ø­ØªØ§Ø¬Ù‡Ø§).\n",
    "\n",
    "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©.\n",
    "\n",
    "ğŸ”¹ 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¹Ø¯ÙŠÙ…Ø© Ø§Ù„Ù‚ÙŠÙ…Ø© (Stopwords Removal)\n",
    "\n",
    "ÙƒÙ„Ù…Ø§Øª Ø²ÙŠ \"the, is, in, Ù…Ù†ØŒ Ø¹Ù„Ù‰\" ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø´ Ø¨ØªØ¶ÙŠÙ Ù…Ø¹Ù†Ù‰ ÙƒØ¨ÙŠØ±.\n",
    "\n",
    "ğŸ”¹ 4. ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ (Tokenization)\n",
    "\n",
    "ØªÙ‚Ø³Ù… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø£Ùˆ Ø¬ÙÙ…Ù„.\n",
    "\n",
    "Ù…Ø«Ø§Ù„: \"I love NLP\" â†’ [\"I\", \"love\", \"NLP\"].\n",
    "\n",
    "ğŸ”¹ 5. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ØµÙŠØºÙ‡Ø§ Ø§Ù„Ø¬Ø°Ø±ÙŠØ© Ø£Ùˆ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "Stemming: Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ù„Ø¬Ø°Ø±Ù‡Ø§ (e.g., \"playing\" â†’ \"play\").\n",
    "\n",
    "Lemmatization: Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ù„ØµÙŠØºØªÙ‡Ø§ Ø§Ù„ØµØ­ÙŠØ­Ø© (e.g., \"better\" â†’ \"good\").\n",
    "\n",
    "ğŸ”¹ 6. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ù„ØªÙ…Ø«ÙŠÙ„ Ø¹Ø¯Ø¯ÙŠ (Feature Extraction)\n",
    "\n",
    "Ø¹Ø´Ø§Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠÙÙ‡Ù…ØŒ Ø§Ù„Ù†Øµ Ù„Ø§Ø²Ù… ÙŠØªØ­ÙˆÙ„ Ù„Ø£Ø±Ù‚Ø§Ù…:\n",
    "\n",
    "Bag of Words (BoW)\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "\n",
    "Transformers Embeddings (BERT, GPT â€¦)\n",
    "\n",
    "ğŸ”¹ 7. Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØºÙŠØ± Ù…ØªÙˆØ§Ø²Ù†Ø© Ø£Ùˆ Ø·ÙˆÙŠÙ„Ø©\n",
    "\n",
    "Padding/Truncating: Ø¹Ø´Ø§Ù† Ø§Ù„Ù†ØµÙˆØµ ÙŠÙƒÙˆÙ† Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ø·ÙˆÙ„.\n",
    "\n",
    "Oversampling/Undersampling: Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ ÙƒÙ„Ø§Ø³ Ù…ÙˆØ²ÙˆÙ† Ø¨Ø´ÙƒÙ„ Ø³ÙŠØ¡.\n",
    "\n",
    "ğŸ”¹ 8. Ø§Ù„ØªÙ‚Ø³ÙŠÙ… (Train-Test Split)\n",
    "\n",
    "ØªÙ‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€:\n",
    "\n",
    "ØªØ¯Ø±ÙŠØ¨ (Train)\n",
    "\n",
    "ØªØ­Ù‚Ù‚ (Validation)\n",
    "\n",
    "Ø§Ø®ØªØ¨Ø§Ø± (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f20c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprhansion \n",
    "#lambda function\n",
    "#adding join to join chars into words \n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lower to teh remove_punc function\n",
    "\n",
    "#list comprhansion \n",
    "#lambda function\n",
    "#adding join to join chars into words \n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_nopunc'] = data['body_text'].apply(lambda x: remove_punct(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc76e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#\\W+ regex, indicates that it will split wherever it sees one or more non-word characters.\n",
    "#So that'll split on white spaces, special characters, anything like that.\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "data['body_text_tokenized'] = data['body_text_nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7abd49",
   "metadata": {},
   "source": [
    "3.4 Remove stopwords\n",
    "Stopwords are common words that are present in the text but generally do not contribute to the meaning of a sentence. They hold almost no importance for the purposes of information retrieval and natural language processing. They can safely be ignored without sacrificing the meaning of the sentence. For example â€“ â€˜theâ€™ and â€˜aâ€™.\n",
    "\n",
    "Stop Words: A stop word is a commonly used word (such as â€œtheâ€, â€œaâ€, â€œanâ€, â€œinâ€) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "The NLTK package has a separate package of stop words that can be downloaded. NLTK has stop words more than 16 languages which can be downloaded and used. Once it is downloaded, it can be passed as an argument indicating it to ignore these words.\n",
    "\n",
    "import nltk  from nltk.corpus \n",
    "import stopwords  set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_En = nltk.corpus.stopwords.words('english')\n",
    "stopwords_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37754f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_En]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bfe40",
   "metadata": {},
   "source": [
    "ğŸ”¹ 1. Stemming\n",
    "\n",
    "Ù‡Ùˆ Ø¹Ù…Ù„ÙŠØ© Ù‚Øµ Ø£Ùˆ ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ø´Ø§Ù† Ù†ÙˆØµÙ„ Ù„Ø¬Ø°Ø±Ù‡Ø§ (Root)ØŒ Ù„ÙƒÙ† Ø¨Ø´ÙƒÙ„ Ø¨Ø³ÙŠØ· ÙˆØ³Ø±ÙŠØ¹ Ù…Ù† ØºÙŠØ± Ù…Ø§ ÙŠÙ‡ØªÙ… Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ù„ØºÙˆÙŠ.\n",
    "\n",
    "Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø³Ø§Ø¹Ø§Øª Ø¨ØªÙƒÙˆÙ† ÙƒÙ„Ù…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙØ¹Ù„ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³.\n",
    "\n",
    "âœ… Ù…Ø«Ø§Ù„:\n",
    "\n",
    "\"Studies\" â†’ \"Studi\"\n",
    "\n",
    "\"Studying\" â†’ \"Study\" Ø£Ùˆ Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ \"Studi\"\n",
    "\n",
    "\"Better\" â†’ \"Better\" (Ù…Ù…ÙƒÙ† ÙŠØ³ÙŠØ¨Ù‡Ø§ Ø²ÙŠ Ù…Ø§ Ù‡ÙŠ Ù„Ø£Ù† Ø§Ù„Ù€ stemmer Ù…Ø´ ÙØ§Ù‡Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰)\n",
    "\n",
    "ğŸ“Œ Ø£Ø¯Ø§Ø© Ù…Ø´Ù‡ÙˆØ±Ø©: Porter Stemmer Ø£Ùˆ Snowball Stemmer ÙÙŠ Ù…ÙƒØªØ¨Ø© NLTK.\n",
    "\n",
    "Ù…ÙŠØ²Ø©: Ø£Ø³Ø±Ø¹.\n",
    "\n",
    "Ø¹ÙŠØ¨: Ù…Ù…ÙƒÙ† ØªØ·Ù„Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù…Ø´ Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ ÙƒÙ„Ù…Ø§Øª Ù…Ø§Ù„Ù‡Ø§Ø´ Ù…Ø¹Ù†Ù‰.\n",
    "\n",
    "ğŸ”¹ 2. Lemmatization\n",
    "\n",
    "Ø¨ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ (Dictionary) + Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºÙˆÙŠØ© (Morphology).\n",
    "\n",
    "Ø¨ØªØ¯ÙŠÙƒ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù„ÙŠ Ù…ÙˆØ¬ÙˆØ¯ ÙØ¹Ù„Ù‹Ø§ ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³.\n",
    "\n",
    "Ø¨ØªØ§Ø®Ø¯ ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ù‡Ø§ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© (POS Tag: verb, noun, adjective, ...).\n",
    "\n",
    "âœ… Ù…Ø«Ø§Ù„:\n",
    "\n",
    "\"Studies\" â†’ \"Study\" (Ø§Ø³Ù… Ø£Ùˆ ÙØ¹Ù„)\n",
    "\n",
    "\"Studying\" â†’ \"Study\"\n",
    "\n",
    "\"Better\" â†’ \"Good\" (Ù„Ø£Ù†Ù‡Ø§ ÙØ§Ù‡Ù…Ø© Ø¥Ù† \"better\" Ù‡ÙŠ ØµÙŠØºØ© ØªÙØ¶ÙŠÙ„ Ù…Ù† \"good\")\n",
    "\n",
    "ğŸ“Œ Ø£Ø¯Ø§Ø© Ù…Ø´Ù‡ÙˆØ±Ø©: WordNet Lemmatizer ÙÙŠ NLTK Ø£Ùˆ spacy Lemmatizer.\n",
    "\n",
    "Ù…ÙŠØ²Ø©: Ø£Ø¯Ù‚.\n",
    "\n",
    "Ø¹ÙŠØ¨: Ø£Ø¨Ø·Ø£ Ù…Ù† stemming Ù„Ø£Ù†Ù‡Ø§ Ù…Ø­ØªØ§Ø¬Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØºÙˆÙŠØ©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6630041",
   "metadata": {},
   "source": [
    "ğŸ”‘ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n",
    "\n",
    "Ù„Ùˆ Ø¹Ø§ÙŠØ² Ø³Ø±Ø¹Ø© (Ø²ÙŠ ÙÙŠ search engines) â†’ Ø§Ø³ØªØ®Ø¯Ù… Stemming.\n",
    "\n",
    "Ù„Ùˆ Ø¹Ø§ÙŠØ² Ø¯Ù‚Ø© ÙˆÙ…Ø¹Ù†Ù‰ ØµØ­ÙŠØ­ (Ø²ÙŠ NLP applications Ø£Ùˆ chatbots) â†’ Ø§Ø³ØªØ®Ø¯Ù… Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3298c",
   "metadata": {},
   "source": [
    "lemmatizer=>Ø§Ø¯Ù‚ Ù Ø§Ù„Ù…Ø¹Ù†ÙŠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_lemma(text):\n",
    "    # 1. Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØªØ­ÙˆÙŠÙ„ Ù„Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©\n",
    "    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    # 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù„ÙƒÙ„Ù…Ø§Øª\n",
    "    tokens = word_tokenize(text)\n",
    "    # 3. Ø¥Ø²Ø§Ù„Ø© stopwords + Lemmatization\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text_lemma(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem=Ø§Ø³Ø±Ø¹\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     #tokens = re.split('\\W+', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_En])\n",
    "    return text\n",
    "data=data[['label','body_text']]\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15023f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc28bd",
   "metadata": {},
   "source": [
    "ğŸ”‘ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n",
    "\n",
    "Ù„Ùˆ Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø³ÙŠØ· (ØªØµÙ†ÙŠÙ Spam/Not Spam Ù…Ø«Ù„Ø§Ù‹) â†’ Ø§Ø³ØªØ®Ø¯Ù… TF-IDF.\n",
    "\n",
    "Ù„Ùˆ Ù…Ø´Ø±ÙˆØ¹ Ø¹Ø§ÙŠØ² Ù…Ø¹Ù†Ù‰ Ø£Ø¹Ù…Ù‚ (Sentiment Analysis, Chatbot, Translation) â†’ Ø§Ø³ØªØ®Ø¯Ù… Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c50b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "#tfidf = TfidfVectorizer(min_df=1)\n",
    "tfidf = TfidfVectorizer( ngram_range=(2,2))\n",
    "features_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n', features_tfidf)\n",
    "features_tfidf = pd.DataFrame(features_tfidf.toarray())\n",
    "features_tfidf.columns = tfidf.get_feature_names_out()\n",
    "features_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
